[[cascading]]
== Cascading support

[quote, Cascading website]
____
http://www.cascading.org/[Cascading] is a data processing API and processing query planner used for defining, sharing, and executing data-processing workflows on a single computing node or distributed computing cluster. 
____

Cascading abstracts the {mr} API and focuses on http://docs.cascading.org/cascading/2.5/userguide/htmlsingle/#N201FA[data processing] 
in terms of 'tuples' http://docs.cascading.org/cascading/2.5/userguide/htmlsingle/#N20C4D['flowing'] through http://docs.cascading.org/cascading/2.5/userguide/htmlsingle/#N2024D['pipes'] between http://docs.cascading.org/cascading/2.5/userguide/htmlsingle/#N2087A['taps'], 
from input (called `SourceTap`) to output (named `SinkTap`). As the data flows, various operations are applied to the tuple; the whole system being transformed to {mr} operations at runtime.
With {eh}, {es} can be plugged into Cascading flows as a `SourceTap` or `SinkTap` through `EsTap`, data to/from {es} being transparently converted from/to Cascading `tuple`s.

****
.Local or Hadoop mode?
Cascading supports two 'execution' modes or http://docs.cascading.org/cascading/2.5/userguide/htmlsingle/#N2081B[platforms]:

Local:: for unit testing and quick POCs. Everything runs only on the local machine and file-system.
Hadoop:: production mode - connects to a proper Hadoop (1.x or 2.x) cluster (as oppose to the 'local' mode which is running just on the local machine).

{eh} supports *both* platforms automatically. One does not have to choose between different classes, `EsTap` can be used as both `sink` or `source`, in both modes transparently.
****

[float]
=== Installation

Just like other libraries, {eh} needs to be available in the jar classpath (either by being manually deployed in the cluster or shipped along with the Hadoop job).

[float]
=== Configuration

[float]
==== Global configuration
Cascading is configured through a `Map<Object, Object>`, typically a `Properties` object which indicates the various Cascading settings and also the application jar:

[source,java]
----
Properties props = new Properties();
AppProps.setApplicationJarClass(props, Main.class);
FlowConnector flow = new HadoopFlowConnector(props);
----

{eh} options can be specified in the same way, these being picked up automatically by _all_ `EsTap`s down the flow:

[source,java]
----
Properties props = new Properties();
props.setProperty("es.index.auto.create", "false"); <1>
...
FlowConnector flow = new HadoopFlowConnector(props);
----

<1> set {eh} option

This approach can be used for local and remote/Hadoop flows - simply use the appropriate `FlowConnector`.

[float]
==== Per-`Tap` configuration
If a `flow` contains multiple {es} taps, the global approach does not work since the settings will clash with each other.  For these scenario, {eh} allows using per-`Tap` configuration:

[source,java]
----
Tap books = new EsTap("es-server", 9200, "my-col/books", "?q=potter");
Tap movies = new EsTap("es-server", 9200, "my-col/movies", "?q=terminator");
----

Note that the `Tap` configuration is merged with the global one so one can mix and match accordingly - for example specify the defaults in the global configuration and only declare the specifics on the `Tap` instance. Additionally, for maximum flexibility `EsTap` allows multiple arguments to be passed in, including a `Properties` object for the full range of options.

[[cascading-alias]]
[float]
=== Mapping

By default, {eh} uses the Cascading tuple to map the data in {es}, using both the field names and types in the process. There are cases however when the field names cannot be used directly with {es} (common case when working with an existing flow). For such cases, one can use the `es.mapping.names` setting which accepts a comma-separated list of names mapping in the following format: `Cascading field name`:`Elasticsearch field name`

To wit:

[source,java]
----
Properties myTapCfg = new Properties();
myTapCfg.set("es.mapping.names", "date:@timestamp");	<1>
Tap myTap = new EsTap(..., myTapCfg);
----

<1> Maps Cascading field `date` in {es} to `@timestamp`

TIP: Since {eh} 2.1, the connector preserves the tuple names case sensitivity.

[[cascading-writing]]
[float]
=== Writing data to {es}

Simply hook, `EsTap` into the Cascading flow:

[source,java]
----
Tap in = new Lfs(new TextDelimited(new Fields("id", "name", "url", "picture")), 
                 "/resources/artists.dat");
Tap out = new EsTap("radio/artists" <1>, new Fields("name", "url", "picture") <2>);
new HadoopFlowConnector().connect(in, out, new Pipe("write-to-Es")).complete();
----

<1> {eh} resource (index and type)
<2> Cascading tuple declaration

For cases where the id (or other metadata fields like +ttl+ or +timestamp+) of the document needs to be specified, one can do so by setting the appropriate <<cfg-mapping, mapping>> namely +es.mapping.id+. Following the previous example, to indicate to {es} to use the field +id+ as the document id, update the +Tap+ configuration:

[source,java]
----
Properties myTapCfg = new Properties();
myTapCfg.set("es.mapping.id", "id");
----

[float]
==== Writing existing JSON to {es}

When the job input data is already in JSON, {eh} allows direct indexing _without_ applying any transformation; the data is taken as is and sent directly to {es}. In such cases, one needs to indicate the json input by setting
the `es.input.json` parameter. As such, in this case {eh} expects to receive a tuple with a single field (representing the JSON document); the library will recognize `Text` or `BytesWritable` types otherwise it just
calls `toString` to get a hold of the JSON content.

IMPORTANT: Make sure the data is properly encoded, in `UTF-8`. The job output is considered the final form of the document sent to {es}.

[source,java]
----
Properties props = new Properties();
...
props.setProperty("es.input.json", "true");                                   <1>
Tap in = new Lfs(new TextLine(new Fields("line")),"/resources/artists.json"); <2>
Tap out = new EsTap("json-cascading-local/artists");
FlowConnector flow = new HadoopFlowConnector(props);
flow.connect(in, out, new Pipe("import-json")).complete();
----

<1> Indicate the input is of type JSON
<2> Load the (JSON) data as a single field (`line`)

[float]
==== Writing to dynamic/multi-resources

One can index the data to a different resource, depending on the tuple being read, by using patterns. Reusing the aforementioned <<cfg-multi-writes,media example>>, one could configure it as follows:

[source,java]
----
Tap out = new EsTap("my-collection/{media.type}" <1>, 
                    new Fields("name", "media.type", "year") <2>);
----

<1> Resource pattern using field `media.type`
<2> Schema definition associated with the `Tap`. Any of the declared fields can be used (example uses `media.type`)

For each tuple about to be written, {eh} will extract the `media.type` entry and use its value to determine the target resource.

The functionality is available when dealing with raw JSON as well - in this case, the value will be extracted from the JSON document itself. Assuming the JSON source contains documents with the following structure:

[source,js]
----
{
    "media_type":"book",<1>
    "title":"Harry Potter",
    "year":"2010"
}
----
<1> field within the JSON document that will be used by the pattern

the `Tap` declaration can be as follows:

[source,java]
----
props.setProperty("es.input.json", "true");                                     
Tap in = new Lfs(new TextLine(new Fields("line")),"/archives/collection.json");
Tap out = new EsTap("my-collection/{media_type}" <1>, new Fields("line") <2>);
----

<1> Resource pattern relying on fields _within_ the JSON document and _not_ on the `Tap` schema
<2> Schema declaration for the `Tap`. Since JSON input is used, the schema is simply a holder to the raw data

[float]
=== Reading data from {es}

Just the same, add `EsTap` on the other end of a pipe, to read (instead of writing) to it.

[source,java]
----
Tap in = new EsTap("radio/artists/"<1>,"?q=me*"<2>);
Tap out = new StdOut(new TextLine());
new LocalFlowConnector().connect(in, out, new Pipe("read-from-Es")).complete();
----

<1> {eh} resource (index and type)
<2> {eh} query


[float]
=== Type conversion

Depending on the http://docs.cascading.org/cascading/2.1/userguide/htmlch03s04.html[platform] used, Cascading can use internally either `Writable` or JDK types for its tuples. {es} handles both transparently 
(see the {mr} <<type-conversion-writable,conversion>> section) though we recommend using the same types (if possible) in both cases to avoid the overhead of maintaining two different versions.

IMPORTANT: If automatic index creation is used, please review <<auto-mapping-type-loss,this>> section for more information.

[float]
=== Cascading Lingual

{eh} also provides integration with http://www.cascading.org/projects/lingual/[Lingual], a Cascading extension 
that provides an ANSI SQL interface for Apache Hadoop. That is, one can execute in Hadoop, SQL queries directly on {es}.

Below is a quick setup of using {eh} with Lingual (1.1) - for detailed information please refer to the Lingual http://docs.cascading.org/lingual/1.1/[user guide]:

[source,bash]
----
export LINGUAL_PLATFORM=hadoop
# register {es} as a provider
lingual catalog --init
lingual catalog --provider --add ./elasticsearch-hadoop-<version>.jar
# add a custom schema (called 'titles') for querying
lingual catalog --schema es-test --add
lingual catalog --schema es-test --stereotype titles -add \
    --columns emp_no,title,from_date,to_date --types int,string,date,date
lingual catalog --schema es-test --format es --add --provider es
lingual catalog --schema es-test --protocol es --add --provider es \
    --properties=host=es-server
lingual catalog --schema es-test --table titles --stereotype titles \
    -add employees/titles --format es --provider es --protocol es
----

Once the desired catalog has been declared and {eh} registered with it, one can start querying the data

[source,sql]
----
lingual shell
(shell) select count(*) from "es-test"."titles" where "title" = 'Engineer';
115003
----
