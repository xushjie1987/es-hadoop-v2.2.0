description = 'Elasticsearch HDFS Repository'

configurations {
    hadoop1
    hadoop2
    s3
}

dependencies {
    provided("org.apache.hadoop:hadoop-core:$hadoop12Version")
    provided("org.elasticsearch:elasticsearch:$esVersion")
    testCompile "org.elasticsearch:elasticsearch:$esVersion:tests"
    testCompile "org.apache.lucene:lucene-test-framework:$luceneVersion"

    hadoop1("org.apache.hadoop:hadoop-core:$hadoop12Version") {
        exclude module: "commons-cli"
        exclude group: "com.sun.jersey"
        exclude group: "org.mortbay.jetty"
        exclude group: "tomcat"
        exclude module: "commons-el"
        exclude module: "hsqldb"
        exclude group: "org.eclipse.jdt"
        exclude module: "commons-beanutils"
        exclude module: "commons-beanutils-core"
        exclude module: "junit"
    }

    hadoop2("org.apache.hadoop:hadoop-client:$hadoop2Version") {
        exclude module: "commons-cli"
        exclude group: "com.sun.jersey"
        exclude group: "com.sun.jersey.contribs"
        exclude group: "com.sun.jersey.jersey-test-framework"
        exclude module: "guice"
        exclude group: "org.mortbay.jetty"
        exclude group: "tomcat"
        exclude module: "commons-el"
        exclude module: "hsqldb"
        exclude group: "org.eclipse.jdt"
        exclude module: "commons-beanutils"
        exclude module: "commons-beanutils-core"
        exclude module: "javax.servlet"
        exclude module: "guava"
        exclude module: "junit"
    }
    
    hadoop2("org.apache.hadoop:hadoop-hdfs:$hadoop2Version") {
        exclude module: "guava"
        exclude module: "log4j"
        exclude module: "junit"
    }
}

tasks.withType(Test) {
    // only set this property on non windows machines
    if (!System.getProperty("os.name").contains("dows")) {
        systemProperty 'java.io.tmpdir', './temp'
    }
}

compileJava {
    sourceCompatibility = 1.7
    targetCompatibility = 1.7 
}

jar {
    manifest.attributes['Implementation-Title'] = 'elasticsearch-repository-hdfs'
    include "org/elasticsearch/plugin/hadoop/hdfs/*"
}

task hadoopLinkedJar(type: Jar, dependsOn:jar) {
    appendix "internal"
    from sourceSets.main.output.classesDir
    // exclude plugin
    exclude "org/elasticsearch/plugin/hadoop/hdfs/*"
}


def baseZip(Zip zip) {
    zip.group = "Distribution"

    artifacts { s3 zip }

    zip.from("../") {
            include "LICENSE.txt"
            include "NOTICE.txt"
            //expand(yyyy: new Date().format("yyyy"), version: project.version)
        }
        
    zip.from (".") { include "README.md" }
    zip.from (".") { 
        include "plugin-descriptor.properties"
        include "plugin-security.policy"
        expand (version: project.version, esVersion: project.esVersion,
            jmvCompatibility: project.targetCompatibility,
            description: project.description, 
            name: project.name)
    }
    zip.from jar.archivePath
    zip.into ("internal-libs") {
        from hadoopLinkedJar.archivePath
    }

    // execute phase
    zip << {
        ant.checksum(file: zip.archivePath, algorithm: 'SHA1', format: 'MD5SUM', fileext: '.sha1.txt')
    }
}

task distZipHadoop1(type: Zip, dependsOn: [hadoopLinkedJar, jar]) { zipTask ->
    baseZip(zipTask)
    description = "Builds archive (with Hadoop1 dependencies) suitable for download page."

    into ("hadoop-libs") {
        from configurations.hadoop1.allArtifacts.files
        from configurations.hadoop1
    }
}

task distZipHadoop2(type: Zip, dependsOn: [hadoopLinkedJar, jar]) { zipTask ->
    baseZip(zipTask)
    description = "Builds archive (with Hadoop2/YARN dependencies) suitable for download page."
    classifier = "hadoop2"

    into ("hadoop-libs") {
        from configurations.hadoop2.allArtifacts.files
        from configurations.hadoop2
    }
}

task distZipNoHadoop(type: Zip, dependsOn: [hadoopLinkedJar, jar]) { zipTask ->
    baseZip(zipTask)
    description = "Builds archive (without any Hadoop dependencies) suitable for download page."
    classifier = "light"
}

task distZip(dependsOn : [distZipHadoop1, distZipHadoop2, distZipNoHadoop]) {
    group = "Distribution"
    description = "Builds all distribution zips for Elasticsearch Repository HDFS"
}

pack() {
    artifacts {
        archives distZipHadoop1
        archives distZipHadoop2
        archives distZipNoHadoop
    }
}

uploadToS3() {
    ext.toDir = "elasticsearch/elasticsearch-repository-hdfs"
}

assemble.dependsOn = ['jar', 'hadoopLinkedJar']
defaultTasks 'build'